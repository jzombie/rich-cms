<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <title>
   Mixture of Experts: How to Properly Scale LLMs
  </title>
  <link href="../../styles/main.css" rel="stylesheet"/>
  <link href="../../styles/pygments.css" rel="stylesheet"/>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css" rel="stylesheet"/>
  <link href="../../images/thinker.png" rel="icon" type="image/png"/>
  <script src="../../scripts/main.js">
  </script>
  <meta content="https://github.com/jzombie/rich-cms" name="generator"/>
  <meta content="2024-06-21 02:54:22 +0000" name="build_datetime"/>
  <meta content="449" name="word_count"/>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
 </head>
 <body>
  <a class="no-css" href="#main-content">
   Skip to main content
  </a>
  <button aria-label="Toggle TOC" class="hamburger js-only" id="menu-toggle-button">
  </button>
  <aside>
   <header>
    <div>
     Mixture of Experts: How to Properly Scale LLMs
    </div>
   </header>
   <nav class="toc">
    <ul>
     <li>
      <ul>
       <li>
        <a href="../../index.html">
         Welcome to zenOSmosis
        </a>
       </li>
       <li>
        <a href="../../Disclaimer.html">
         Disclaimer
        </a>
       </li>
      </ul>
     </li>
     <li>
      <ul>
       <li class="directory">
        <div class="label">
         <a href="../../Think%20and%20Grow%20Rich/index.html">
          Think and Grow Rich
         </a>
        </div>
        <ul>
         <li>
          <a href="../../Think%20and%20Grow%20Rich/index.html">
           Copyright / General Info
          </a>
         </li>
         <li>
          <a href="../../Think%20and%20Grow%20Rich/FOREWORD.html">
           FOREWORD
          </a>
         </li>
         <li>
          <a href="../../Think%20and%20Grow%20Rich/Chapter%201.html">
           CHAPTER 1
          </a>
         </li>
         <li>
          <a href="../../Think%20and%20Grow%20Rich/Chapter%202.html">
           CHAPTER 2
          </a>
         </li>
         <li>
          <a href="../../Think%20and%20Grow%20Rich/Chapter%203.html">
           CHAPTER 3
          </a>
         </li>
         <li>
          <a href="../../Think%20and%20Grow%20Rich/Chapter%204.html">
           CHAPTER 4
          </a>
         </li>
         <li>
          <a href="../../Think%20and%20Grow%20Rich/Chapter%205.html">
           CHAPTER 5
          </a>
         </li>
         <li>
          <a href="../../Think%20and%20Grow%20Rich/Chapter%206.html">
           CHAPTER 6
          </a>
         </li>
         <li>
          <a href="../../Think%20and%20Grow%20Rich/Chapter%207.html">
           CHAPTER 7
          </a>
         </li>
         <li>
          <a href="../../Think%20and%20Grow%20Rich/Chapter%208.html">
           CHAPTER 8
          </a>
         </li>
         <li>
          <a href="../../Think%20and%20Grow%20Rich/Chapter%209.html">
           CHAPTER 9
          </a>
         </li>
         <li>
          <a href="../../Think%20and%20Grow%20Rich/Chapter%2010.html">
           CHAPTER 10
          </a>
         </li>
         <li>
          <a href="../../Think%20and%20Grow%20Rich/Chapter%2011.html">
           CHAPTER 11
          </a>
         </li>
         <li>
          <a href="../../Think%20and%20Grow%20Rich/Chapter%2012.html">
           CHAPTER 12
          </a>
         </li>
         <li>
          <a href="../../Think%20and%20Grow%20Rich/Chapter%2013.html">
           CHAPTER 13
          </a>
         </li>
         <li>
          <a href="../../Think%20and%20Grow%20Rich/Chapter%2014.html">
           CHAPTER 14
          </a>
         </li>
         <li>
          <a href="../../Think%20and%20Grow%20Rich/Chapter%2015.html">
           CHAPTER 15
          </a>
         </li>
        </ul>
       </li>
      </ul>
     </li>
    </ul>
   </nav>
   <footer>
    <nav>
     <a class="previous" href="Links%20of%20Interest.html">
      « Previous
     </a>
     <a class="next" href="Pyrhon%20via%20WASM.html">
      Next »
     </a>
    </nav>
   </footer>
  </aside>
  <article id="main-content">
   <nav>
    <a href="../../index.html">
     Home
    </a>
    &gt;
    <a href="../../Miscellaneous/">
     Miscellaneous
    </a>
    &gt;
    <a href="../../Miscellaneous/Nerdy%20Stuff/A%20Tester%20Walks%20Into%20A%20Bar.html">
     Nerdy Stuff
    </a>
    &gt; Mixture of Experts: How to Properly Scale LLMs
   </nav>
   <p>
    Estimated reading time: 2 minute(s).
   </p>
   <div>
    <h1 id="mixture-of-experts-how-to-properly-scale-llms">
     Mixture of Experts: How to Properly Scale LLMs
    </h1>
    <p>
     Reference:
     <a href="https://www.linkedin.com/posts/damienbenveniste_these-days-you-blink-and-there-are-20-new-activity-7178417751282081793-EyiP?utm_source=share&amp;utm_medium=member_desktop" rel="noopener noreferrer" target="_blank">
      https://www.linkedin.com/posts/damienbenveniste_these-days-you-blink-and-there-are-20-new-activity-7178417751282081793-EyiP?utm_source=share&amp;utm_medium=member_desktop
     </a>
    </p>
    <p class="has-drop-cap">
     These days, you blink, and there are 20 new Machine Learning techniques or tools you need to learn about! The Mixture of Experts architecture is by no means a new technique, but it has now become the default strategy for scaling LLMs. I remember reading about it a couple of years ago and dismissing it as "yet another LLM paper that probably doesn't matter". Well, now it matters! Most larger LLMs are likely to use that strategy moving forward!
    </p>
    <p>
     The typical transformer block is a succession of an attention layer, layer normalization, feed-forward layer, and another layer normalization. The strategy to scale transformers has been to just add more transformer blocks one after the other. The idea with MoE is to scale "horizontally" by adding more parallel feed-forward layers in each of the blocks. Those are the "experts".
    </p>
    <p>
     Prior to the experts layer, we add a router so that each token only goes through a few experts. For example, we can have 64 experts, but with the token's hidden states only going through 2 of those. This ensures diverse learning while minimizing the computational load and, therefore, latency at inference time.
    </p>
    <p>
     The router is just a linear layer that takes a hidden state and produces a vector with as many entries as there are experts. By using a softmax transformation, we get a probability for each of the experts. We can now use those probabilities to select the top-k experts and build a weighted average of the output of the selected experts. For example, if we take the top 2 experts:
    </p>
    <pre><code class="language-python">new state = P(FFN_1) * FFN_1 (hidden state) + P(FFN_2) * FFN_2 (hidden state)
</code></pre>
    <p>
     Even with only the top-2 experts, the new output hidden state can represent a much richer set of information learned by the different combinations of experts. This also provides a very natural way to distribute the model computations across multiple GPU machines. Each machine can hold multiple experts, and the computations of the different experts can happen in parallel on the different machines.
    </p>
    <p>
     However, training a MoE model is not trivial as it induces a lot of training instabilities. One difficulty is ensuring each expert sees enough data to learn the relevant statistical patterns. The typical strategy is adding a term to the loss function to provide a balanced data load across experts.
    </p>
    <p>
     Google has been leading the charge on that front, so these are worth a read:
    </p>
    <ul>
     <li>
      <a href="https://arxiv.org/pdf/2202.08906.pdf" rel="noopener noreferrer" target="_blank">
       ST-MOE: Designing Stable and Transferable
Sparse Expert Models
      </a>
     </li>
     <li>
      <a href="https://arxiv.org/pdf/2101.03961.pdf" rel="noopener noreferrer" target="_blank">
       Switch Transformers: Scaling to Trillion Parameter Models
with Simple and Efficient Sparsity
      </a>
     </li>
    </ul>
    <p>
     <img alt="Mixture of Experts: How to Properly Scale LLMs" src="../images/moe-scaling-llms.jpeg"/>
    </p>
   </div>
   <footer>
    <div class="build_datetime">
     Build datetime: 2024-06-21 02:54:22 +0000
    </div>
    <nav>
     <a class="previous" href="Links%20of%20Interest.html">
      « Previous
     </a>
     <a class="next" href="Pyrhon%20via%20WASM.html">
      Next »
     </a>
    </nav>
   </footer>
  </article>
 </body>
</html>
